# -*- coding: utf-8 -*-
"""Trustpilot Review Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L0QUEs5adUG-GdNTSsIgGzjkH815A37I

# Data Loading and understanding the Data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('/content/data.csv')

df.head()

df = df[['rating','rating title','Review text','Review date','Date of Experience','rating_procesed','Year of review ','Year of experience','DIff in months ']]

df.head(2)

df.shape

df['rating'].value_counts()

"""# Data Visulisation"""

issues = df['Review text'].tolist()

issues

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Vectorize the text data
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(issues)

# Apply K-Means clustering
num_clusters = 5  # Specify the number of clusters
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
kmeans.fit(X)

# Get cluster labels
cluster_labels = kmeans.labels_

# Get the top terms for each cluster
def get_top_terms_per_cluster(kmeans, vectorizer, n_terms=8):
    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()
    cluster_terms = []
    for i in range(kmeans.n_clusters):
        cluster_terms.append([terms[ind] for ind in order_centroids[i, :n_terms]])
    return cluster_terms

top_terms = get_top_terms_per_cluster(kmeans, vectorizer)
for i, terms in enumerate(top_terms):
    print(f"Cluster {i}: {', '.join(terms)}")

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Reduce dimensions with t-SNE
tsne = TSNE(n_components=2, random_state=42,init="random")
X_tsne = tsne.fit_transform(X)

# Plot the clusters
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_labels, cmap='viridis')
plt.title("t-SNE Visualization of Clusters")
plt.show()

"""## Using Transfer Learning"""

from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, TensorDataset, random_split

# Load pre-trained BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_clusters)

# Tokenize the text data
inputs = tokenizer(issues, padding=True, truncation=True, return_tensors='pt')

import torch

# Create a dataset and data loader
dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], torch.tensor(cluster_labels))
train_size = int(0.8 * len(dataset))
train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

# Define the optimizer
optimizer = AdamW(model.parameters(), lr=2e-5)

"""## Using Topic Modelling to get high Level and Low level labels for the data"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import normalize

reviews = df['Review text'].values

# Preprocess the text (tokenization, removing stopwords, etc.)
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X = vectorizer.fit_transform(reviews)

"""### Finding High level labels"""

from sklearn.decomposition import LatentDirichletAllocation

# Fit LDA model
num_topics = 10  # Define the number of high-level topics
lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
X_topics = lda.fit_transform(X)

# Get the top words for each topic
def get_top_words(model, feature_names, n_top_words):
    top_words = []
    for topic_idx, topic in enumerate(model.components_):
        top_words.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])
    return top_words

feature_names = vectorizer.get_feature_names_out()
top_words = get_top_words(lda, feature_names, 10)
print(top_words)

"""## Clustering for subtopics"""

from sklearn.cluster import KMeans

# Perform clustering within each topic
num_subtopics = 3  # Define the number of subtopics per topic
subtopic_labels = []

for topic_idx in range(num_topics):
    topic_reviews = X_topics[:, topic_idx]
    topic_indices = topic_reviews.argsort()[-500:]  # Get top 500 reviews for the topic
    topic_data = X[topic_indices]
    kmeans = KMeans(n_clusters=num_subtopics, random_state=42)
    subtopic_label = kmeans.fit_predict(topic_data)
    subtopic_labels.append(subtopic_label)



print(subtopic_labels)

# Perform clustering within each topic
num_subtopics = 2  # Define the number of subtopics per topic
subtopic_labels = []

# Initialize a list to store all topic indices for mapping later
all_topic_indices = []

for topic_idx in range(num_topics):
    topic_reviews = X_topics[:, topic_idx]
    topic_indices = topic_reviews.argsort()[-(len(reviews) // num_topics):]  # Get top indices for the topic
    all_topic_indices.append(topic_indices)  # Collect all topic indices

    topic_data = X[topic_indices]
    kmeans = KMeans(n_clusters=num_subtopics, random_state=42)
    subtopic_label = kmeans.fit_predict(topic_data)
    subtopic_labels.append(subtopic_label)

# Flatten the list of topic indices
flat_topic_indices = [idx for sublist in all_topic_indices for idx in sublist]

# Map subtopics to original reviews
review_subtopic_labels = [-1] * len(reviews)
for topic_idx, indices in enumerate(all_topic_indices):
    for i, review_idx in enumerate(indices):
        review_subtopic_labels[review_idx] = subtopic_labels[topic_idx][i]

# Print the resulting labels
print("Review Subtopic Labels:")
print(review_subtopic_labels)

# Combine topic and subtopic labels
hierarchical_labels = []
for i in range(len(reviews)):
    main_topic = np.argmax(X_topics[i])
    subtopic = review_subtopic_labels[i]
    hierarchical_labels.append((main_topic, subtopic))

hierarchical_labels

# Convert to a more readable format
label_names = ['Topic_' + str(i) for i in range(num_topics)]
subtopic_names = [['Subtopic_' + str(i) for i in range(num_subtopics)] for _ in range(num_topics)]

label_names

subtopic_names

hierarchical_labels

human_readable_labels = []
for main_topic, subtopic in hierarchical_labels:
    if subtopic != -1:
        label = f"{label_names[main_topic]} -> {subtopic_names[main_topic][subtopic]}"
    else:
        label = label_names[main_topic]
    human_readable_labels.append(label)

human_readable_labels

# Add the hierarchical labels to the DataFrame
df['hierarchical_label'] = human_readable_labels
print(df[['Review text', 'hierarchical_label']])

df

# Function to get top terms for each subtopic
def get_top_terms_per_cluster(kmeans, vectorizer, n_terms=3):
    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()
    cluster_terms = []
    for i in range(kmeans.n_clusters):
        cluster_terms.append([terms[ind] for ind in order_centroids[i, :n_terms]])
    return cluster_terms

# Assign reviews to clusters
df_cluster = pd.DataFrame({'Review text': reviews, 'cluster': cluster_labels})

vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X = vectorizer.fit_transform(reviews)

# Fit LDA model
num_topics = 5  # Define the number of high-level topics
lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
X_topics = lda.fit_transform(X)

# Get the top words for each topic
def get_top_words(model, feature_names, n_top_words):
    top_words = []
    for topic_idx, topic in enumerate(model.components_):
        top_words.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])
    return top_words

feature_names = vectorizer.get_feature_names_out()
top_words = get_top_words(lda, feature_names, 10)
for i, words in enumerate(top_words):
    print(f"Topic {i}: {', '.join(words)}")

from sklearn.cluster import KMeans

# Perform clustering within each topic
num_subtopics = 2  # Define the number of subtopics per topic

# Iterate through each topic
for topic_idx in range(num_topics):
    topic_reviews = X_topics[:, topic_idx]
    topic_indices = topic_reviews.argsort()[-len(reviews) // num_topics:]  # Adjusted for balance
    topic_data = X[topic_indices]

    kmeans = KMeans(n_clusters=num_subtopics, random_state=42)
    kmeans.fit(topic_data)

    # Get the cluster labels
    cluster_labels = kmeans.labels_

    # Get the top terms for each cluster
    def get_top_terms_per_cluster(kmeans, vectorizer, cluster_labels, n_terms=5):
        order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
        terms = vectorizer.get_feature_names_out()
        cluster_terms = []
        for i in range(num_subtopics):
            cluster_terms.append([terms[ind] for ind in order_centroids[i, :n_terms]])
        return cluster_terms

    cluster_terms = get_top_terms_per_cluster(kmeans, vectorizer, cluster_labels)
    for j, terms in enumerate(cluster_terms):
        print(f"Topic {topic_idx} - Subtopic {j}: {', '.join(terms)}")



# Display the clustered reviews
for cluster in range(num_clusters):
    print(f"\nCluster {cluster}:")
    for review in df_cluster[df_cluster['cluster'] == cluster]['Review text']:
        print(f" - {review}")

df.head()

# @title Review date vs rating_procesed

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['Review date']
  ys = series['rating_procesed']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = df.sort_values('Review date', ascending=True)
for i, (series_name, series) in enumerate(df_sorted.groupby('rating')):
  _plot_series(series, series_name, i)
  fig.legend(title='rating', bbox_to_anchor=(1, 1), loc='upper left')
sns.despine(fig=fig, ax=ax)
plt.xlabel('Review date')
_ = plt.ylabel('rating_procesed')

df['hierarchical_label'] = human_readable_labels

df.head()

# @title Distribution of Ratings Over Time

import matplotlib.pyplot as plt

# Group the data by year and rating and count the occurrences
rating_distribution = df.groupby(['Year of review ', 'rating_procesed'])['rating_procesed'].count().unstack()

# Plot the stacked bar chart
rating_distribution.plot(kind='bar', stacked=True)
plt.xlabel('Year')
plt.ylabel('Number of Ratings')
_ = plt.title('Distribution of Ratings Over Time')

# @title Distribution of Time Difference Between Review and Experience

import matplotlib.pyplot as plt

# Plot the histogram
plt.hist(df['DIff in months '], bins=15)
plt.xlabel('Difference in Months (Review Date - Experience Date)')
plt.ylabel('Frequency')
_ = plt.title('Distribution of Time Difference')

"""# Model Training"""

from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df['hierarchical_label'])

y

from sklearn.model_selection import train_test_split

X = df['Review text']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

y_train



y_train

# Vectorize the reviews
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['Review text'])

# Cluster the reviews to create main topics
kmeans_main = KMeans(n_clusters=3, random_state=42)
main_topic_labels = kmeans_main.fit_predict(X)

# Add main topic labels to the DataFrame
df['main_topic'] = main_topic_labels

df.head()

# Sub-cluster the reviews within each main topic
subtopic_labels = []
for main_topic in df['main_topic'].unique():
    indices = df[df['main_topic'] == main_topic].index
    sub_X = X[indices]

    kmeans_sub = KMeans(n_clusters=2, random_state=42)
    sub_labels = kmeans_sub.fit_predict(sub_X)

    for i, idx in enumerate(indices):
        subtopic_labels.append((idx, sub_labels[i]))

# Sort subtopic labels to match DataFrame order
subtopic_labels = sorted(subtopic_labels, key=lambda x: x[0])
subtopic_labels = [label[1] for label in subtopic_labels]

# Add subtopic labels to the DataFrame
df['sub_topic'] = subtopic_labels

df.head()

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['Review text'])  # Fit the vectorizer and transform the data

# Function to get top terms for each cluster
def get_top_terms_per_cluster(kmeans, vectorizer, n_terms=5):
    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()
    top_terms = {}
    for i in range(kmeans.n_clusters):
        top_terms[i] = [terms[ind] for ind in order_centroids[i, :n_terms]]
    return top_terms

# Cluster the reviews to create main topics
kmeans_main = KMeans(n_clusters=3, random_state=42)
main_topic_labels = kmeans_main.fit_predict(X)

# Get top terms for main topics
main_topic_terms = get_top_terms_per_cluster(kmeans_main, vectorizer)

# Map main topic labels to terms
df['main_topic_label'] = [main_topic_terms[label] for label in main_topic_labels]

# Sub-cluster the reviews within each main topic
subtopic_labels = []
subtopic_terms = {}
for main_topic in df['main_topic_label'].unique():
    indices = df[df['main_topic_label'] == main_topic].index
    sub_X = X[indices]

    kmeans_sub = KMeans(n_clusters=2, random_state=42)
    sub_labels = kmeans_sub.fit_predict(sub_X)

    # Get top terms for subtopics
    sub_topic_terms = get_top_terms_per_cluster(kmeans_sub, vectorizer)
    subtopic_terms[tuple(main_topic)] = sub_topic_terms

    for i, idx in enumerate(indices):
        subtopic_labels.append((idx, sub_labels[i]))

# Sort subtopic labels to match DataFrame order
subtopic_labels = sorted(subtopic_labels, key=lambda x: x[0])
subtopic_labels = [label[1] for label in subtopic_labels]

# Add subtopic labels to the DataFrame
df['sub_topic_label'] = subtopic_labels
df['sub_topic_label'] = df.apply(lambda row: subtopic_terms[tuple(row['main_topic_label'])][row['sub_topic_label']], axis=1)

# Combine main and subtopic labels to create hierarchical labels
df['hierarchical_label'] = df.apply(lambda row: ' -> '.join(row['main_topic_label']) + ' -> ' + ' -> '.join(row['sub_topic_label']), axis=1)

# Display the DataFrame with hierarchical labels
print(df[['review_text', 'hierarchical_label']])

# Function to get top terms for each cluster
def get_top_terms_per_cluster(kmeans, vectorizer, n_terms=5):
    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()
    top_terms = {}
    for i in range(kmeans.n_clusters):
        top_terms[i] = [terms[ind] for ind in order_centroids[i, :n_terms]]
    return top_terms

# Cluster the reviews to create main topics
kmeans_main = KMeans(n_clusters=3, random_state=42)
main_topic_labels = kmeans_main.fit_predict(X)

# Get top terms for main topics
main_topic_terms = get_top_terms_per_cluster(kmeans_main, vectorizer)

# Map main topic labels to terms
df['main_topic_label'] = main_topic_labels

# Sub-cluster the reviews within each main topic
subtopic_labels = []
subtopic_terms = {}
for main_topic_label in range(kmeans_main.n_clusters):
    indices = df[df['main_topic_label'] == main_topic_label].index
    sub_X = X[indices]

    if sub_X.shape[0] > 1:  # Proceed only if there's more than one sample to cluster
        kmeans_sub = KMeans(n_clusters=2, random_state=42)
        sub_labels = kmeans_sub.fit_predict(sub_X)

        # Get top terms for subtopics
        sub_topic_terms = get_top_terms_per_cluster(kmeans_sub, vectorizer)
        subtopic_terms[main_topic_label] = sub_topic_terms

        for i, idx in enumerate(indices):
            subtopic_labels.append((idx, sub_labels[i]))
    else:
        subtopic_labels.append((indices[0], 0))  # Assign a single cluster if there's only one sample

# Sort subtopic labels to match DataFrame order
subtopic_labels = sorted(subtopic_labels, key=lambda x: x[0])
subtopic_labels = [label[1] for label in subtopic_labels]

# Add subtopic labels to the DataFrame
df['sub_topic_label'] = subtopic_labels

# Create a mapping for subtopics to terms
df['sub_topic_label'] = df.apply(lambda row: subtopic_terms.get(row['main_topic_label'], {}).get(row['sub_topic_label'], []), axis=1)

# Combine main and subtopic labels to create hierarchical labels
df['hierarchical_label'] = df.apply(lambda row: ' -> '.join(main_topic_terms[row['main_topic_label']]) + ' -> ' + ' -> '.join(row['sub_topic_label']), axis=1)

# Display the DataFrame with hierarchical labels
print(df[['Review text', 'hierarchical_label']])

df.head()

# Vectorize the reviews
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['Review text'])  # Fit the vectorizer and transform the data

# Function to get top terms for each cluster
def get_top_terms_per_cluster(kmeans, vectorizer, n_terms=5):
    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()
    top_terms = {}
    for i in range(kmeans.n_clusters):
        top_terms[i] = [terms[ind] for ind in order_centroids[i, :n_terms]]
    return top_terms

# Cluster the reviews to create main topics
kmeans_main = KMeans(n_clusters=3, random_state=42)
main_topic_labels = kmeans_main.fit_predict(X)

# Get top terms for main topics
main_topic_terms = get_top_terms_per_cluster(kmeans_main, vectorizer)

# Map main topic labels to terms
df['main_topic_label'] = [main_topic_terms[label] for label in main_topic_labels]

# Sub-cluster the reviews within each main topic
subtopic_labels = []
subtopic_terms = {}
for main_topic in df['main_topic_label']:
    indices = df[df['main_topic_label'] == main_topic].index
    sub_X = X[indices]

    kmeans_sub = KMeans(n_clusters=2, random_state=42)
    sub_labels = kmeans_sub.fit_predict(sub_X)

    # Get top terms for subtopics
    sub_topic_terms = get_top_terms_per_cluster(kmeans_sub, vectorizer)
    subtopic_terms[tuple(main_topic)] = sub_topic_terms

    for i, idx in enumerate(indices):
        subtopic_labels.append((idx, sub_labels[i]))

# Sort subtopic labels to match DataFrame order
subtopic_labels = sorted(subtopic_labels, key=lambda x: x[0])
subtopic_labels = [label[1] for label in subtopic_labels]

# Add subtopic labels to the DataFrame
df['sub_topic_label'] = subtopic_labels
df['sub_topic_label'] = df.apply(lambda row: subtopic_terms[tuple(row['main_topic_label'])][row['sub_topic_label']], axis=1)

# Combine main and subtopic labels to create hierarchical labels
df['hierarchical_label'] = df.apply(lambda row: ' -> '.join(row['main_topic_label']) + ' -> ' + ' -> '.join(row['sub_topic_label']), axis=1)

# Display the DataFrame with hierarchical labels
print(df[['review_text', 'hierarchical_label']])

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans



# Vectorize the reviews
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(df['Review text'])  # Fit the vectorizer and transform the data

# Function to get top terms for each cluster
def get_top_terms_per_cluster(kmeans, vectorizer, n_terms=5):
    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
    terms = vectorizer.get_feature_names_out()
    top_terms = {}
    for i in range(kmeans.n_clusters):
        top_terms[i] = [terms[ind] for ind in order_centroids[i, :n_terms]]
    return top_terms

# Cluster the reviews to create main topics
kmeans_main = KMeans(n_clusters=3, random_state=42)
main_topic_labels = kmeans_main.fit_predict(X)

# Get top terms for main topics
main_topic_terms = get_top_terms_per_cluster(kmeans_main, vectorizer)

# Map main topic labels to terms
df['main_topic_label'] = main_topic_labels

# Sub-cluster the reviews within each main topic
subtopic_labels = []
subtopic_terms = {}
for main_topic_label in range(kmeans_main.n_clusters):
    indices = df[df['main_topic_label'] == main_topic_label].index
    sub_X = X[indices]

    if sub_X.shape[0] > 1:  # Proceed only if there's more than one sample to cluster
        kmeans_sub = KMeans(n_clusters=2, random_state=42)
        sub_labels = kmeans_sub.fit_predict(sub_X)

        # Get top terms for subtopics
        sub_topic_terms = get_top_terms_per_cluster(kmeans_sub, vectorizer)
        subtopic_terms[main_topic_label] = sub_topic_terms

        for i, idx in enumerate(indices):
            subtopic_labels.append((idx, sub_labels[i]))
    else:
        subtopic_labels.append((indices[0], 0))  # Assign a single cluster if there's only one sample

# Sort subtopic labels to match DataFrame order
subtopic_labels = sorted(subtopic_labels, key=lambda x: x[0])
subtopic_labels = [label[1] for label in subtopic_labels]

# Add subtopic labels to the DataFrame
df['sub_topic_label'] = subtopic_labels

# Create a mapping for subtopics to terms
df['sub_topic_label'] = df.apply(lambda row: subtopic_terms.get(row['main_topic_label'], {}).get(row['sub_topic_label'], []), axis=1)

# Combine main and subtopic labels to create hierarchical labels
df['hierarchical_label'] = df.apply(lambda row: ' -> '.join(main_topic_terms[row['main_topic_label']]) + ' -> ' + ' -> '.join(row['sub_topic_label']), axis=1)

# Display the DataFrame with hierarchical labels
print(df[['review_text', 'hierarchical_label']])

from matplotlib import pyplot as plt
import seaborn as sns
import pandas as pd
plt.subplots(figsize=(8, 8))
df_2dhist = pd.DataFrame({
    x_label: grp['rating title'].value_counts()
    for x_label, grp in _df_20.groupby('rating')
})
sns.heatmap(df_2dhist, cmap='viridis')
plt.xlabel('rating')
_ = plt.ylabel('rating title')

# prompt: write code to train a model so that it is able to predict the labels from the given review text.write complete code, I want to train it on the df dataset above. Also generate the training and test dataset from the df dataframe. since the task of language modelling use, some language modelling relating technique. There can be multiple labels. So treat it as multi class classification problme

import pandas as pd
from sklearn.model_selection import train_test_split

# Prepare the data
X = df['Review text']
y = df['main_topic_label']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Vectorize the text data
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(stop_words='english')
X_train = vectorizer.fit_transform(X_train)
X_test = vectorizer.transform(X_test)

mlb = MultiLabelBinarizer()
y_train = mlb.fit_transform(y_train)
y_test = mlb.transform(y_test)

# Train the model
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC

model = OneVsRestClassifier(LinearSVC())
model.fit(X_train, y_train)

# Evaluate the model
from sklearn.metrics import accuracy_score

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

df.head()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MultiLabelBinarizer


# MultiLabelBinarizer to convert hierarchical labels to binary array format
mlb = MultiLabelBinarizer()
y_sub_labels = mlb.fit_transform(df['sub_topic_label'].apply(lambda x: [x]))

# Split the data into training and testing sets
X_train, X_test,y_sub_train, y_sub_test = train_test_split(
    df['Review text'],  y_sub_labels, test_size=0.2, random_state=42)

# Vectorize the review texts
vectorizer = TfidfVectorizer(stop_words='english')
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)


# Train a classifier for sub-topics
sub_topic_clf = LogisticRegression(random_state=42)
sub_topic_clf.fit(X_train_tfidf, y_sub_train)

# Predict on the test set
sub_topic_pred = sub_topic_clf.predict(X_test_tfidf)

# Calculate accuracy
sub_topic_accuracy = accuracy_score(y_sub_test, sub_topic_pred)

print(f"Sub-Topic Accuracy: {sub_topic_accuracy}")

